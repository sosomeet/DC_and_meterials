{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49ecef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using columns: ['d_UST10Y', 'dlog_COPPER', 'dlog_DXY', 'dlog_SOLVPN', 'dlog_VIX']\n",
      "[INFO] Date range: 2020-10-13 00:00:00 ~ 2026-01-12 00:00:00\n",
      "[INFO] n = 1325\n",
      "[INFO] Selected lag p=1 by HQIC (maxlags=8, forced p>=1=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] GIRF saved: girf_long.csv and GIRF__shock_*.png in ./spillover_outputs\n",
      "[INFO] H=7 GFEVD saved: gfevd_H7.csv\n",
      "[INFO] H=7 DY saved: dy_table_H7.csv, dy_total_H7.txt\n",
      "[INFO] H=15 GFEVD saved: gfevd_H15.csv\n",
      "[INFO] H=15 DY saved: dy_table_H15.csv, dy_total_H15.txt\n",
      "[INFO] H=30 GFEVD saved: gfevd_H30.csv\n",
      "[INFO] H=30 DY saved: dy_table_H30.csv, dy_total_H30.txt\n",
      "[DONE] Spillover outputs are in: ./spillover_outputs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Spillover Interpretation Pipeline (VAR -> GIRF, GFEVD, DY)\n",
    "# FIXED:\n",
    "#   - Prevent p=0 (force p>=1)\n",
    "#   - Use HQIC by default for interpretation stability\n",
    "#   - Optional: set business-day frequency (off by default)\n",
    "#\n",
    "# Outputs (./spillover_outputs):\n",
    "#   - var_summary.txt\n",
    "#   - girf_long.csv\n",
    "#   - GIRF__shock_*.png\n",
    "#   - gfevd_H{H}.csv\n",
    "#   - dy_table_H{H}.csv\n",
    "#   - dy_total_H{H}.txt\n",
    "#   - run_metadata.csv\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "IN_PATH = \"./only_var_input.csv\"\n",
    "OUT_DIR = \"./spillover_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "USE_DIFF_COLS_IF_PRESENT = True\n",
    "\n",
    "MAXLAGS = 8\n",
    "LAG_SELECT = \"hqic\"     # \"hqic\"(recommended) or \"aic\" or \"bic\"\n",
    "FORCE_MIN_LAG_1 = True  # <<< 핵심: p=0 방지\n",
    "\n",
    "IRF_PERIODS = 30\n",
    "FEVD_HORIZONS = [7, 15, 30]\n",
    "\n",
    "# Set to True only if you really want to force a business-day frequency.\n",
    "# If your series skips holidays/weekends, leaving this False is fine.\n",
    "FORCE_BUSINESS_DAY_FREQ = False\n",
    "FILL_METHOD_IF_FREQ_FORCED = \"ffill\"  # \"ffill\" or None\n",
    "\n",
    "# Standardize before estimation? Usually unnecessary for GIRF/GFEVD on dlog series.\n",
    "STANDARDIZE = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def choose_var_columns(df, use_diff_like=True):\n",
    "    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if use_diff_like:\n",
    "        diff_like = [c for c in numeric_cols if c.startswith(\"dlog_\") or c.startswith(\"d_\")]\n",
    "        cols = diff_like if len(diff_like) >= 2 else numeric_cols\n",
    "    else:\n",
    "        cols = numeric_cols\n",
    "    cols = sorted(cols)\n",
    "    if len(cols) < 2:\n",
    "        raise ValueError(f\"Need >=2 numeric columns for VAR. Found: {cols}\")\n",
    "    return cols\n",
    "\n",
    "def standardize_fit_apply(X):\n",
    "    mu = X.mean()\n",
    "    sd = X.std(ddof=0).replace(0, 1.0)\n",
    "    Xz = (X - mu) / sd\n",
    "    return Xz, mu, sd\n",
    "\n",
    "def safe_int(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            return None\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -------------------------\n",
    "# Generalized IRF (Pesaran-Shin)\n",
    "# GIRF_h(:,j) = Phi_h * Sigma * e_j / sqrt(sigma_jj)\n",
    "# -------------------------\n",
    "def compute_girf(var_res, periods=30):\n",
    "    Sigma = np.asarray(var_res.sigma_u)        # (K,K)\n",
    "    K = Sigma.shape[0]\n",
    "    Phi = var_res.ma_rep(periods)              # (periods+1, K, K), includes h=0\n",
    "\n",
    "    girf = np.zeros((periods + 1, K, K), dtype=float)  # h, response_i, shock_j\n",
    "    for j in range(K):\n",
    "        ej = np.zeros((K, 1))\n",
    "        ej[j, 0] = 1.0\n",
    "        denom = np.sqrt(Sigma[j, j]) if Sigma[j, j] > 0 else 1.0\n",
    "        for h in range(periods + 1):\n",
    "            girf[h, :, j] = (Phi[h] @ Sigma @ ej).flatten() / denom\n",
    "    return girf\n",
    "\n",
    "# -------------------------\n",
    "# Generalized FEVD (Pesaran-Shin / DY-style)\n",
    "# theta_ij(H) = (1/sigma_jj) * sum_{h=0..H-1} (e_i' Phi_h Sigma e_j)^2\n",
    "#               / sum_{h=0..H-1} e_i' Phi_h Sigma Phi_h' e_i\n",
    "# Row-normalize so each i sums to 1 (DY convention).\n",
    "# -------------------------\n",
    "def compute_gfevd(var_res, horizon):\n",
    "    Sigma = np.asarray(var_res.sigma_u)\n",
    "    K = Sigma.shape[0]\n",
    "    Phi = var_res.ma_rep(horizon - 1)          # 0..H-1 length H\n",
    "\n",
    "    num = np.zeros((K, K), dtype=float)\n",
    "    den = np.zeros(K, dtype=float)\n",
    "\n",
    "    diag_sigma = np.diag(Sigma)\n",
    "    diag_sigma = np.where(diag_sigma > 0, diag_sigma, 1.0)\n",
    "\n",
    "    for h in range(horizon):\n",
    "        Ph = Phi[h]\n",
    "        PhSigma = Ph @ Sigma\n",
    "\n",
    "        den += np.diag(Ph @ Sigma @ Ph.T)\n",
    "\n",
    "        # (e_i' Ph Sigma e_j)^2 / sigma_jj\n",
    "        num += (PhSigma ** 2) / diag_sigma[None, :]\n",
    "\n",
    "    den = np.where(den > 0, den, 1e-12)\n",
    "    theta = num / den[:, None]\n",
    "\n",
    "    row_sums = theta.sum(axis=1, keepdims=True)\n",
    "    row_sums = np.where(row_sums > 0, row_sums, 1e-12)\n",
    "    theta_norm = theta / row_sums\n",
    "    return theta_norm\n",
    "\n",
    "def dy_spillover_table(theta_norm, var_names):\n",
    "    K = len(var_names)\n",
    "    off_diag_sum = theta_norm.sum() - np.trace(theta_norm)\n",
    "    total = 100.0 * off_diag_sum / K\n",
    "\n",
    "    to_others = 100.0 * (theta_norm.sum(axis=1) - np.diag(theta_norm)) / K\n",
    "    from_others = 100.0 * (theta_norm.sum(axis=0) - np.diag(theta_norm)) / K\n",
    "    net = to_others - from_others\n",
    "\n",
    "    tbl = pd.DataFrame({\n",
    "        \"variable\": var_names,\n",
    "        \"to_others(%)\": to_others,\n",
    "        \"from_others(%)\": from_others,\n",
    "        \"net(%)\": net\n",
    "    })\n",
    "    return total, tbl\n",
    "\n",
    "def plot_girf(girf, var_names, out_dir):\n",
    "    Hp1, K, _ = girf.shape\n",
    "    horizons = np.arange(Hp1)\n",
    "\n",
    "    for shock_j, shock_name in enumerate(var_names):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for resp_i, resp_name in enumerate(var_names):\n",
    "            plt.plot(horizons, girf[:, resp_i, shock_j], label=resp_name)\n",
    "        plt.axhline(0.0, linewidth=1)\n",
    "        plt.title(f\"GIRF responses to 1-s.d. shock in: {shock_name}\")\n",
    "        plt.xlabel(\"Horizon (days)\")\n",
    "        plt.ylabel(\"Response\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, f\"GIRF__shock_{shock_name}.png\"), dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "df = pd.read_csv(IN_PATH)\n",
    "\n",
    "if \"Date\" in df.columns:\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Date\"]).sort_values(\"Date\").reset_index(drop=True)\n",
    "    df = df.set_index(\"Date\")\n",
    "\n",
    "df = df.dropna().copy()\n",
    "\n",
    "var_cols = choose_var_columns(df, use_diff_like=USE_DIFF_COLS_IF_PRESENT)\n",
    "X = df[var_cols].copy()\n",
    "\n",
    "# Optional: enforce frequency\n",
    "if FORCE_BUSINESS_DAY_FREQ and isinstance(X.index, pd.DatetimeIndex):\n",
    "    X = X.asfreq(\"B\")\n",
    "    if FILL_METHOD_IF_FREQ_FORCED == \"ffill\":\n",
    "        X = X.ffill()\n",
    "    X = X.dropna()\n",
    "\n",
    "print(\"[INFO] Using columns:\", var_cols)\n",
    "if isinstance(X.index, pd.DatetimeIndex):\n",
    "    print(\"[INFO] Date range:\", X.index.min(), \"~\", X.index.max())\n",
    "print(\"[INFO] n =\", len(X))\n",
    "\n",
    "# Optional standardization\n",
    "if STANDARDIZE:\n",
    "    Xz, mu, sd = standardize_fit_apply(X)\n",
    "else:\n",
    "    Xz = X\n",
    "    mu = sd = None\n",
    "\n",
    "# Fit VAR\n",
    "model = VAR(Xz)\n",
    "\n",
    "order = model.select_order(MAXLAGS)\n",
    "selected = safe_int(getattr(order, LAG_SELECT, None))\n",
    "\n",
    "# Fallback chain\n",
    "if selected is None:\n",
    "    for crit in [\"hqic\", \"bic\", \"aic\"]:\n",
    "        selected = safe_int(getattr(order, crit, None))\n",
    "        if selected is not None:\n",
    "            LAG_SELECT = crit\n",
    "            break\n",
    "\n",
    "if selected is None:\n",
    "    raise RuntimeError(\"Lag selection failed. Try reducing MAXLAGS or checking data.\")\n",
    "\n",
    "p = int(selected)\n",
    "if FORCE_MIN_LAG_1:\n",
    "    p = max(1, p)\n",
    "\n",
    "print(f\"[INFO] Selected lag p={p} by {LAG_SELECT.upper()} (maxlags={MAXLAGS}, forced p>=1={FORCE_MIN_LAG_1})\")\n",
    "\n",
    "res = model.fit(p)\n",
    "\n",
    "# Save summary\n",
    "with open(os.path.join(OUT_DIR, \"var_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(str(res.summary()))\n",
    "\n",
    "# -------------------------\n",
    "# 1) GIRF\n",
    "# -------------------------\n",
    "girf = compute_girf(res, periods=IRF_PERIODS)  # (H+1, K, K)\n",
    "\n",
    "# Save GIRF long\n",
    "K = len(var_cols)\n",
    "rows = []\n",
    "for h in range(IRF_PERIODS + 1):\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            rows.append({\n",
    "                \"horizon\": h,\n",
    "                \"response\": var_cols[i],\n",
    "                \"shock\": var_cols[j],\n",
    "                \"girf\": float(girf[h, i, j])\n",
    "            })\n",
    "girf_df = pd.DataFrame(rows)\n",
    "girf_df.to_csv(os.path.join(OUT_DIR, \"girf_long.csv\"), index=False)\n",
    "\n",
    "# Plot GIRF\n",
    "plot_girf(girf, var_cols, OUT_DIR)\n",
    "print(f\"[INFO] GIRF saved: girf_long.csv and GIRF__shock_*.png in {OUT_DIR}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) GFEVD + DY\n",
    "# -------------------------\n",
    "for H in FEVD_HORIZONS:\n",
    "    theta = compute_gfevd(res, horizon=H)\n",
    "    fevd = pd.DataFrame(theta, index=var_cols, columns=var_cols)\n",
    "    fevd.to_csv(os.path.join(OUT_DIR, f\"gfevd_H{H}.csv\"))\n",
    "\n",
    "    total, dy_tbl = dy_spillover_table(theta, var_cols)\n",
    "    dy_tbl.to_csv(os.path.join(OUT_DIR, f\"dy_table_H{H}.csv\"), index=False)\n",
    "\n",
    "    with open(os.path.join(OUT_DIR, f\"dy_total_H{H}.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Total spillover index (DY, generalized FEVD), H={H}: {total:.6f}\\n\")\n",
    "\n",
    "    print(f\"[INFO] H={H} GFEVD saved: gfevd_H{H}.csv\")\n",
    "    print(f\"[INFO] H={H} DY saved: dy_table_H{H}.csv, dy_total_H{H}.txt\")\n",
    "\n",
    "# -------------------------\n",
    "# Metadata\n",
    "# -------------------------\n",
    "meta = {\n",
    "    \"IN_PATH\": IN_PATH,\n",
    "    \"OUT_DIR\": OUT_DIR,\n",
    "    \"COLUMNS_USED\": var_cols,\n",
    "    \"N\": int(len(X)),\n",
    "    \"DATE_MIN\": str(X.index.min()) if isinstance(X.index, pd.DatetimeIndex) else \"\",\n",
    "    \"DATE_MAX\": str(X.index.max()) if isinstance(X.index, pd.DatetimeIndex) else \"\",\n",
    "    \"MAXLAGS\": MAXLAGS,\n",
    "    \"LAG_SELECT\": LAG_SELECT,\n",
    "    \"SELECTED_P\": int(p),\n",
    "    \"FORCE_MIN_LAG_1\": FORCE_MIN_LAG_1,\n",
    "    \"IRF_PERIODS\": IRF_PERIODS,\n",
    "    \"FEVD_HORIZONS\": FEVD_HORIZONS,\n",
    "    \"STANDARDIZE\": STANDARDIZE,\n",
    "    \"FORCE_BUSINESS_DAY_FREQ\": FORCE_BUSINESS_DAY_FREQ,\n",
    "    \"RANDOM_STATE\": RANDOM_STATE,\n",
    "}\n",
    "pd.Series(meta).to_csv(os.path.join(OUT_DIR, \"run_metadata.csv\"), header=False)\n",
    "\n",
    "print(f\"[DONE] Spillover outputs are in: {OUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
